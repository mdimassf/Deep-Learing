{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcsTfPpI04lL",
        "outputId": "b29e64bc-98ac-45e4-c48f-91f87d364371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m180.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLibrary berhasil diimport.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install --upgrade pip --quiet\n",
        "!{sys.executable} -m pip install torch torchvision torchtext --quiet\n",
        "!{sys.executable} -m pip install tensorflow --quiet\n",
        "!{sys.executable} -m pip install scikit-learn --quiet\n",
        "!{sys.executable} -m pip install matplotlib --quiet\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             roc_curve, auc)\n",
        "\n",
        "print('Library berhasil diimport.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG54upEY04lM",
        "outputId": "daab4727-4630-45bd-8a79-74b1359a4f30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Train shape: (25000, 300) Label train: (25000,)\n",
            "Test shape : (25000, 300) Label test : (25000,)\n"
          ]
        }
      ],
      "source": [
        "num_words = 30000\n",
        "maxlen = 300\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=num_words)\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "print('Train shape:', x_train.shape, 'Label train:', y_train.shape)\n",
        "print('Test shape :', x_test.shape, 'Label test :', y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYrLleDR04lN",
        "outputId": "2f694416-f4e2-46f9-c59e-f2c019004c9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch models (RNN, LSTM, GRU) defined.\n"
          ]
        }
      ],
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.LongTensor(X)\n",
        "        self.y = torch.LongTensor(y)\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = IMDBDataset(x_train, y_train)\n",
        "test_dataset = IMDBDataset(x_test, y_test)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128, output_dim=1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.rnn(x)\n",
        "        out = out[:, -1, :]\n",
        "        return torch.sigmoid(self.fc(out))\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128, output_dim=1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, (h, c) = self.lstm(x)\n",
        "        out = out[:, -1, :]\n",
        "        return torch.sigmoid(self.fc(out))\n",
        "\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128, output_dim=1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.gru(x)\n",
        "        out = out[:, -1, :]\n",
        "        return torch.sigmoid(self.fc(out))\n",
        "\n",
        "print('PyTorch models (RNN, LSTM, GRU) defined.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xkitrf4Q04lN",
        "outputId": "0bd7625c-552f-4be1-952a-5c57480d23d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch training function defined.\n"
          ]
        }
      ],
      "source": [
        "def train_pytorch_model(model, train_loader, test_loader, epochs=3, lr=0.001, device='cpu'):\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    model.to(device)\n",
        "    train_losses, test_losses = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device).float()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch).squeeze()\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        test_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in test_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device).float()\n",
        "                outputs = model(X_batch).squeeze()\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                test_loss += loss.item()\n",
        "        test_loss = test_loss / len(test_loader)\n",
        "        test_losses.append(test_loss)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "    return train_losses, test_losses\n",
        "\n",
        "print('PyTorch training function defined.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO1PQnFX04lO",
        "outputId": "24e94371-83bd-4917-f61f-fc3254464fc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Epoch [1/3], Train Loss: 0.6492, Test Loss: 0.6132\n",
            "Epoch [2/3], Train Loss: 0.5799, Test Loss: 0.6173\n",
            "Epoch [3/3], Train Loss: 0.5005, Test Loss: 0.5468\n",
            "Epoch [1/3], Train Loss: 0.5788, Test Loss: 0.5640\n",
            "Epoch [2/3], Train Loss: 0.4706, Test Loss: 0.4461\n",
            "Epoch [3/3], Train Loss: 0.3716, Test Loss: 0.4616\n",
            "Epoch [1/3], Train Loss: 0.5656, Test Loss: 0.6591\n",
            "Epoch [2/3], Train Loss: 0.4616, Test Loss: 0.4246\n",
            "Epoch [3/3], Train Loss: 0.2630, Test Loss: 0.3403\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using device:', device)\n",
        "\n",
        "# RNN\n",
        "pytorch_rnn = RNNModel(vocab_size=num_words)\n",
        "rnn_train_losses, rnn_test_losses = train_pytorch_model(\n",
        "    pytorch_rnn, train_loader, test_loader, epochs=3, lr=0.001, device=device\n",
        ")\n",
        "\n",
        "# LSTM\n",
        "pytorch_lstm = LSTMModel(vocab_size=num_words)\n",
        "lstm_train_losses, lstm_test_losses = train_pytorch_model(\n",
        "    pytorch_lstm, train_loader, test_loader, epochs=3, lr=0.001, device=device\n",
        ")\n",
        "\n",
        "# GRU\n",
        "pytorch_gru = GRUModel(vocab_size=num_words)\n",
        "gru_train_losses, gru_test_losses = train_pytorch_model(\n",
        "    pytorch_gru, train_loader, test_loader, epochs=3, lr=0.001, device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EquS6SiA04lO",
        "outputId": "614b19eb-a00b-4618-a758-cdc1d3530d77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow (Keras) model builders defined.\n"
          ]
        }
      ],
      "source": [
        "def create_tf_rnn_model(vocab_size, embed_dim=128, hidden_dim=128):\n",
        "    model = keras.Sequential([\n",
        "        layers.Embedding(vocab_size, embed_dim, input_length=maxlen),\n",
        "        layers.SimpleRNN(hidden_dim, return_sequences=False),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def create_tf_lstm_model(vocab_size, embed_dim=128, hidden_dim=128):\n",
        "    model = keras.Sequential([\n",
        "        layers.Embedding(vocab_size, embed_dim, input_length=maxlen),\n",
        "        layers.LSTM(hidden_dim, return_sequences=False),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def create_tf_gru_model(vocab_size, embed_dim=128, hidden_dim=128):\n",
        "    model = keras.Sequential([\n",
        "        layers.Embedding(vocab_size, embed_dim, input_length=maxlen),\n",
        "        layers.GRU(hidden_dim, return_sequences=False),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "print('TensorFlow (Keras) model builders defined.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7-BRa6lN04lO",
        "outputId": "3fa3b392-2f1f-4ac5-dba0-6ad31f7e5a2f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "ename": "FailedPreconditionError",
          "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-7-0829199000e2>\", line 2, in <cell line: 0>\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_2321]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-0829199000e2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf_rnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tf_rnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history_rnn = tf_rnn.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-7-0829199000e2>\", line 2, in <cell line: 0>\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_2321]"
          ]
        }
      ],
      "source": [
        "tf_rnn = create_tf_rnn_model(num_words)\n",
        "history_rnn = tf_rnn.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=3,\n",
        "    batch_size=64,\n",
        "    validation_data=(x_test, y_test)\n",
        ")\n",
        "\n",
        "tf_lstm = create_tf_lstm_model(num_words)\n",
        "history_lstm = tf_lstm.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=3,\n",
        "    batch_size=64,\n",
        "    validation_data=(x_test, y_test)\n",
        ")\n",
        "\n",
        "tf_gru = create_tf_gru_model(num_words)\n",
        "history_gru = tf_gru.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=3,\n",
        "    batch_size=64,\n",
        "    validation_data=(x_test, y_test)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93ODpQfn04lO"
      },
      "outputs": [],
      "source": [
        "# Fungsi evaluasi PyTorch\n",
        "def evaluate_pytorch(model, data_loader, device='cpu'):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            preds = model(X_batch).squeeze().cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(y_batch.numpy())\n",
        "    return np.array(all_preds), np.array(all_labels)\n",
        "\n",
        "# Fungsi evaluasi TF\n",
        "def evaluate_tf(model, X, y):\n",
        "    preds = model.predict(X).ravel()\n",
        "    return preds, y\n",
        "\n",
        "# Fungsi perhitungan metrik\n",
        "def compute_metrics(probs, labels, threshold=0.5):\n",
        "    preds = (probs >= threshold).astype(int)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    prec = precision_score(labels, preds)\n",
        "    rec = recall_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds)\n",
        "    fpr, tpr, _ = roc_curve(labels, probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'precision': prec,\n",
        "        'recall': rec,\n",
        "        'f1_score': f1,\n",
        "        'auc': roc_auc\n",
        "    }, (fpr, tpr)\n",
        "\n",
        "print('Evaluation functions ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1d_JcFB04lO"
      },
      "source": [
        "### Evaluasi PyTorch Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qYkkKcj04lO"
      },
      "outputs": [],
      "source": [
        "# RNN PyTorch\n",
        "rnn_probs, rnn_labels = evaluate_pytorch(pytorch_rnn, test_loader, device=device)\n",
        "rnn_metrics, (rnn_fpr, rnn_tpr) = compute_metrics(rnn_probs, rnn_labels)\n",
        "print('PyTorch RNN:', rnn_metrics)\n",
        "\n",
        "# LSTM PyTorch\n",
        "lstm_probs, lstm_labels = evaluate_pytorch(pytorch_lstm, test_loader, device=device)\n",
        "lstm_metrics, (lstm_fpr, lstm_tpr) = compute_metrics(lstm_probs, lstm_labels)\n",
        "print('PyTorch LSTM:', lstm_metrics)\n",
        "\n",
        "# GRU PyTorch\n",
        "gru_probs, gru_labels = evaluate_pytorch(pytorch_gru, test_loader, device=device)\n",
        "gru_metrics, (gru_fpr, gru_tpr) = compute_metrics(gru_probs, gru_labels)\n",
        "print('PyTorch GRU:', gru_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAo4nxF404lO"
      },
      "source": [
        "### Evaluasi TensorFlow Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Te8edLH004lO"
      },
      "outputs": [],
      "source": [
        "# RNN TF\n",
        "tf_rnn_probs, tf_rnn_labels = evaluate_tf(tf_rnn, x_test, y_test)\n",
        "tf_rnn_metrics, (tf_rnn_fpr, tf_rnn_tpr) = compute_metrics(tf_rnn_probs, tf_rnn_labels)\n",
        "print('TF RNN:', tf_rnn_metrics)\n",
        "\n",
        "# LSTM TF\n",
        "tf_lstm_probs, tf_lstm_labels = evaluate_tf(tf_lstm, x_test, y_test)\n",
        "tf_lstm_metrics, (tf_lstm_fpr, tf_lstm_tpr) = compute_metrics(tf_lstm_probs, tf_lstm_labels)\n",
        "print('TF LSTM:', tf_lstm_metrics)\n",
        "\n",
        "# GRU TF\n",
        "tf_gru_probs, tf_gru_labels = evaluate_tf(tf_gru, x_test, y_test)\n",
        "tf_gru_metrics, (tf_gru_fpr, tf_gru_tpr) = compute_metrics(tf_gru_probs, tf_gru_labels)\n",
        "print('TF GRU:', tf_gru_metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10S1zlYc04lO"
      },
      "source": [
        "### Visualisasi Loss (Contoh PyTorch RNN dan TensorFlow RNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYqZ4_5904lP"
      },
      "outputs": [],
      "source": [
        "# Visualisasi PyTorch RNN loss\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(rnn_train_losses, label='Train Loss')\n",
        "plt.plot(rnn_test_losses, label='Test Loss')\n",
        "plt.title('PyTorch RNN Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Visualisasi TensorFlow RNN loss\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(history_rnn.history['loss'], label='Train Loss')\n",
        "plt.plot(history_rnn.history['val_loss'], label='Val Loss')\n",
        "plt.title('TensorFlow RNN Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgA4Kwaa1Hzh"
      },
      "source": [
        "Persamaan Metrik Evaluasi\n",
        "\n",
        "Definisi Dasar Klasifikasi\n",
        "\n",
        "True Positive (TP): Jumlah prediksi positif yang benar\n",
        "\n",
        "True Negative (TN): Jumlah prediksi negatif yang benar\n",
        "\n",
        "False Positive (FP): Jumlah prediksi positif yang salah (kesalahan tipe I)\n",
        "\n",
        "False Negative (FN): Jumlah prediksi negatif yang salah (kesalahan tipe II)\n",
        "\n",
        "Accuracy (Akurasi)\n",
        "\n",
        "\n",
        "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
        "\n",
        "\n",
        "Proporsi prediksi yang benar dari seluruh prediksi. Baik untuk dataset seimbang.\n",
        "\n",
        "Accuracy untuk multi-kelas\n",
        "\n",
        "$$\\text{Accuracy} = \\frac{\\text{Jumlah prediksi yang benar}}{\\text{Total jumlah prediksi}}$$\n",
        "\n",
        "\n",
        "Generalisasi akurasi untuk kasus multi-kelas.\n",
        "\n",
        "Precision (Presisi)\n",
        "\n",
        "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
        "\n",
        "Proporsi prediksi positif yang benar dari seluruh prediksi positif. Berguna\n",
        "ketika biaya FP tinggi.\n",
        "\n",
        "Macro-Precision\n",
        "\n",
        "$$\\text{Macro-Precision} = \\frac{1}{C}\\sum_{i=1}^{C} \\text{Precision}_i$$\n",
        "\n",
        "\n",
        "Rata-rata precision dari semua kelas, memberikan bobot yang sama untuk setiap kelas.\n",
        "\n",
        "Recall (Sensitivity)\n",
        "\n",
        "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
        "\n",
        "Proporsi kasus positif yang teridentifikasi dari seluruh kasus positif sebenarnya. Berguna ketika biaya FN tinggi.\n",
        "\n",
        "Macro-Recall\n",
        "\n",
        "$$\\text{Macro-Recall} = \\frac{1}{C}\\sum_{i=1}^{C} \\text{Recall}_i$$\n",
        "\n",
        "Rata-rata recall dari semua kelas, memberikan bobot yang sama untuk setiap kelas.\n",
        "\n",
        "F1 Score\n",
        "\n",
        "$$\\text{F1} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
        "\n",
        "Rata-rata harmonik dari precision dan recall. Memberikan keseimbangan antara kedua metrik tersebut.\n",
        "\n",
        "Macro-F1\n",
        "\n",
        "$$\\text{Macro-F1} = \\frac{1}{C}\\sum_{i=1}^{C} \\text{F1}_i$$\n",
        "\n",
        "Rata-rata F1 score dari semua kelas.\n",
        "\n",
        "Specificity (True Negative Rate)\n",
        "\n",
        "$$\\text{Specificity} = \\frac{TN}{TN + FP}$$\n",
        "\n",
        "Proporsi kasus negatif yang teridentifikasi dengan benar dari seluruh kasus negatif.\n",
        "\n",
        "True Positive Rate (untuk ROC)\n",
        "\n",
        "$$\\text{TPR} = \\frac{TP}{TP + FN} = \\text{Recall}$$\n",
        "\n",
        "Sama dengan Recall, mengukur kemampuan model menemukan semua kasus positif.\n",
        "\n",
        "False Positive Rate (untuk ROC)\n",
        "\n",
        "$$\\text{FPR} = \\frac{FP}{FP + TN} = 1 - \\text{Specificity}$$\n",
        "\n",
        "Proporsi kasus negatif yang salah diklasifikasikan sebagai positif.\n",
        "\n",
        "AUC (Area Under Curve)\n",
        "\n",
        "$$\\text{AUC} = \\int_{0}^{1} \\text{TPR}(\\text{FPR}^{-1}(t)) dt$$\n",
        "\n",
        "Area di bawah kurva ROC, mengukur kemampuan model untuk membedakan antara kelas. Nilai berkisar dari 0.5 (acak) hingga 1 (sempurna).\n",
        "\n",
        "Macro-AUC\n",
        "\n",
        "$$\\text{Macro-AUC} = \\frac{1}{C}\\sum_{i=1}^{C} \\text{AUC}_i$$\n",
        "\n",
        "Rata-rata AUC dari semua kelas dalam kasus multi-kelas.\n",
        "\n",
        "Log Loss (Cross-Entropy Loss)\n",
        "\n",
        "$$\\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{ij} \\log(p_{ij})$$\n",
        "\n",
        "Mengukur performa model probabilistik. Menghukum keras prediksi yang salah dengan keyakinan tinggi. Di mana:\n",
        "\n",
        "N adalah jumlah sampel\n",
        "\n",
        "C adalah jumlah kelas\n",
        "\n",
        "y_ij adalah indikator biner (0 atau 1) jika sampel i termasuk kelas j\n",
        "\n",
        "p_ij adalah probabilitas prediksi bahwa sampel i termasuk kelas j\n",
        "\n",
        "Cohen's Kappa\n",
        "\n",
        "$$\\kappa = \\frac{p_o - p_e}{1 - p_e}$$\n",
        "\n",
        "Mengukur kecocokan yang memperhitungkan kebetulan. Nilai berkisar dari -1 hingga 1, dengan 1 adalah kecocokan sempurna. Di mana:\n",
        "\n",
        "p_o adalah kecocokan relatif yang diamati (akurasi)\n",
        "\n",
        "p_e adalah kecocokan yang diharapkan secara kebetulan\n",
        "\n",
        "Hubungan antar Metrik\n",
        "\n",
        "Metrik-metrik ini saling melengkapi, dan pemilihannya bergantung pada karakteristik masalah:\n",
        "\n",
        "Untuk dataset tidak seimbang, Precision, Recall, F1, dan AUC lebih informatif daripada Accuracy\n",
        "\n",
        "Jika FP lebih bermasalah, fokus pada Precision\n",
        "\n",
        "Jika FN lebih bermasalah, fokus pada Recall\n",
        "F1 Score memberikan keseimbangan antara Precision dan Recall\n",
        "\n",
        "AUC mengukur kemampuan membedakan kelas secara keseluruhan tanpa dipengaruhi threshold\n",
        "\n",
        "Untuk klasifikasi multi-kelas, macro-average (rata-rata sederhana dari semua kelas) sering digunakan untuk memberikan bobot yang sama pada semua kelas, sementara weighted-average mempertimbangkan frekuensi kelas."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
